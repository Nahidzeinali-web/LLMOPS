{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0698b84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama  \n",
    "import pypandoc  # Convert Markdown to DOCX\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pandas as pd  # Handle structured data\n",
    "import os  # File handling\n",
    "from dotenv import load_dotenv  # Load API keys securely\n",
    "# Load environment variables (ensure API keys are stored in a .env file)\n",
    "load_dotenv()\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7c2fc4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llm=ChatGroq(model=\"deepseek-r1-distill-llama-70b\")\n",
    "print(llm.invoke(\"What is the capital of France?\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3f127941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "[0.06777575612068176, 0.8263235092163086, -4.1526618003845215, -0.7515212297439575, 1.320723533630371, 0.8680969476699829, -0.8879498243331909, 1.0088109970092773, -0.08663347363471985, -0.7173395752906799]\n"
     ]
    }
   ],
   "source": [
    "# choose one of the embedding models you pulled\n",
    "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "# embedding_model = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "\n",
    "doc_vector = embedding_model.embed_query(\"What is the capital of France?\")\n",
    "doc_vector\n",
    "print(len(doc_vector))   # size depends on model (e.g., 768 or 1024)\n",
    "print(doc_vector[:10])   # preview first 10 values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ca7015",
   "metadata": {},
   "source": [
    "Use nomic-embed-text or mxbai-embed-large for embeddings.\n",
    "ollama pull mxbai-embed-large\n",
    "\n",
    "Use llama3:8b for lightweight answering, or llama3:70b for best accuracy if you have the hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada4263a",
   "metadata": {},
   "source": [
    "1-Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2800a415",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.getcwd()\n",
    "file_path=os.path.join(os.getcwd(), \"data\", \"sample.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
